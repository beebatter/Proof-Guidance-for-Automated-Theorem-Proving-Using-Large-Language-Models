# Proof-Guidance-for-Automated-Theorem-Proving-Using-Large-Language-Models

Automated theorem provers (ATPs) such as iProver or E‐Prover can exhaustively search a space of clauses to produce mechanically‐correct proofs—sometimes spanning hundreds or thousands of resolution steps. However, this search often blows up combinatorially, leading to very long runtimes or outright timeouts. Meanwhile, large language models (LLMs) exhibit surprisingly strong “heuristic reasoning” abilities: they can propose plausible next steps or rank premises, but they lack formal guarantees and may quickly diverge if left unverified.

This project aims to integrate LLM‐based “proof guidance” into a saturation‐based ATP (iProver/E‐Prover) so that, at each Given‐Clause selection point, the LLM proposes or ranks the next clause to resolve. By steering the prover toward promising inferences, we hope to reduce search effort (fewer generated clauses, faster proof discovery) while preserving mechanical soundness (falling back to classical heuristics if the LLM suggestion appears invalid). Over an 8‐week development period, we will (1) prepare the codebase and data pipelines, (2) survey existing “ML‐guided” ATP literature, (3) design / implement an LLM‐callback interface for Given‐Clause selection, (4) optimize caching and fall‐back strategies, and (5) evaluate end‐to‐end performance on TPTP benchmarks, comparing pure‐ATP, LLM‐guided, and hybrid settings.
